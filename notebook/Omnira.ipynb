{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc2SvHlgLP_J"
      },
      "source": [
        "###Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk5VitodxQv_",
        "outputId": "7f462b05-c1ee-465f-bba0-b18764d4a37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgrYC2g_xDBJ"
      },
      "source": [
        "###configuration file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "djwQ4krXwH0M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"lr\": 1e-4,\n",
        "        \"seq_len\": 512,\n",
        "        \"d_model\": 768,\n",
        "        \"n_layers\": 12,\n",
        "        \"head\": 12,\n",
        "        \"d_ff\": 3072,\n",
        "        \"dropout\": 0.1,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_k\": 13900,\n",
        "        \"vocab_size\": 0,\n",
        "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"model_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM2.pt\",\n",
        "        \"dataset_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/dataset.json\",\n",
        "        \"tokenizer_file_path\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/tokenizer.json\",\n",
        "        \"logs\": \"/content/drive/MyDrive/Colab Notebooks/T-CLM/T-CLM-log\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2a7fTMsxGUH"
      },
      "source": [
        "###BPE Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yv9S_PQcwj0i"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "def get_tokenizer(config):\n",
        "    tokenizer_path = Path(config['tokenizer_file_path'])\n",
        "    if tokenizer_path.exists():\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    else:\n",
        "        print(\"tokenizer file not found!\")\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1LIieN_LbB5"
      },
      "source": [
        "###Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1VPhoImUwpcQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import json\n",
        "\n",
        "class MultiTurnChatDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "        self.user_token = torch.tensor([tokenizer.token_to_id(\"[USER]\")], dtype=torch.int64)\n",
        "        self.bot_token = torch.tensor([tokenizer.token_to_id(\"[BOT]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.ds[idx]['conversation']\n",
        "\n",
        "        # Concatenate multi-turn dialogue, adding <USER> and <BOT> tokens for each turn\n",
        "        conversation_tokens = []\n",
        "        for i, turn in enumerate(conversation):\n",
        "            if i % 2 == 0:  # Even index: User turn\n",
        "                conversation_tokens += [self.user_token] + self.tokenizer.encode(turn).ids\n",
        "            else:  # Odd index: Bot turn\n",
        "                conversation_tokens += [self.bot_token] + self.tokenizer.encode(turn).ids\n",
        "\n",
        "        # Ensure the total length doesn't exceed the sequence length\n",
        "        num_padding_tokens = self.seq_len - len(conversation_tokens)\n",
        "        if num_padding_tokens < 0:\n",
        "            raise ValueError(\"Multi-turn conversation is too long for the sequence length\")\n",
        "\n",
        "        # Pad the conversation tokens\n",
        "        input_tokens = conversation_tokens + [self.pad_token] * num_padding_tokens\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        inputs = torch.tensor(input_tokens, dtype=torch.int64)\n",
        "\n",
        "        assert inputs.size(0) == self.seq_len\n",
        "\n",
        "        # The model's task is to predict the next bot response, so the labels are shifted\n",
        "        labels = torch.tensor(input_tokens[1:] + [self.pad_token], dtype=torch.int64)  # Shifted by 1 for language modeling\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs,\n",
        "            \"labels\": labels,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFjxO_cGMVg5"
      },
      "source": [
        "###Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MQyv27OIMG5J"
      },
      "outputs": [],
      "source": [
        "def get_ds(config):\n",
        "    with open(config['dataset_file_path'], 'r', encoding='utf-8') as f:\n",
        "        ds_raw = json.load(f)\n",
        "\n",
        "    tokenizer = get_tokenizer(config)\n",
        "\n",
        "    # Split the dataset into train and validation sets\n",
        "    train_ds_size = int(0.99 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_ds = MultiTurnChatDataset(train_ds_raw, tokenizer, config['seq_len'])\n",
        "    val_ds = MultiTurnChatDataset(val_ds_raw, tokenizer, config['seq_len'])\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI2RCCppxLNv"
      },
      "source": [
        "###Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ycm0_YdhwnJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        if dropout is not None:\n",
        "            scores = dropout(scores)\n",
        "        return scores @ value, scores\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, attn = self.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList, norm_layer: LayerNormalization) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "class TCLM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, seq_len: int, d_model: int, N: int, h: int, dropout: float, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.input_embed = InputEmbeddings(d_model, vocab_size)\n",
        "        self.pos_embed = PositionalEncoding(d_model, seq_len, dropout)\n",
        "\n",
        "        # Decoder blocks with multi-head attention and feed-forward\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(\n",
        "                features=d_model,\n",
        "                self_attention_block=MultiHeadAttentionBlock(d_model, h, dropout),\n",
        "                feed_forward_block=FeedForwardBlock(d_model, d_ff, dropout),\n",
        "                dropout=dropout\n",
        "            ) for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        self.decoder = Decoder(self.layers, LayerNormalization(d_model))\n",
        "        self.projection_layer = ProjectionLayer(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        token_embed = self.input_embed(idx)\n",
        "        x = self.pos_embed(token_embed)\n",
        "\n",
        "        tgt_mask = torch.tril(torch.ones((T, T), device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
        "        x = self.decoder(x, tgt_mask)\n",
        "\n",
        "        logits = self.projection_layer(x)\n",
        " \n",
        "        if targets is not None:\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int, seq_len: int, temperature: float = 1.0, top_k: int = 10):\n",
        "        vocab_size = self.projection_layer.proj.out_features\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            if idx.size(1) > seq_len:\n",
        "                idx_crop = idx[:, -seq_len:]\n",
        "            else:\n",
        "                idx_crop = idx\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits, _ = self.forward(idx_crop)\n",
        "            logits = logits[:, -1, :]  # logits for the last token in the sequence\n",
        "\n",
        "            logits = logits / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            top_k = min(top_k, vocab_size)\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "        \n",
        "            top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
        "            idx_next = top_k_indices.gather(-1, torch.multinomial(top_k_probs, 1))\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Z-sKhGvQSc"
      },
      "source": [
        "###Preload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sjl15GUkvS8m"
      },
      "outputs": [],
      "source": [
        "def load_model(config, device, model, optimizer):\n",
        "    initial_epoch = 1\n",
        "    global_step = 0\n",
        "    model = model.to(device)\n",
        "    model_file_path = config['model_file_path']\n",
        "    if Path(model_file_path).exists():\n",
        "        print(f'Loading model from {str(model_file_path)}')\n",
        "        state = torch.load(str(model_file_path), map_location=device, weights_only=True)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print(\"No model file found.\")\n",
        "\n",
        "    return model, optimizer, initial_epoch, global_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHEZMF_edPIc"
      },
      "source": [
        "###WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR1KEZ50ZGuW"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em-UtNUhWRl9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPOUVLnjKz2-"
      },
      "source": [
        "###Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmm5oPx4wr3U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import warnings\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "def train(config):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"T-CLM2\", config=config)\n",
        "\n",
        "    device = config['device']\n",
        "    print(\"Using device:\", device)\n",
        "    if device == 'cuda':\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {round(torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3, 1)} GB\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer = get_ds(config)\n",
        "    model = TCLM(vocab_size=tokenizer.get_vocab_size(), seq_len=config['seq_len'], d_model=config['d_model'], N=config['n_layers'], h=config['head'], dropout=config['dropout'], d_ff=config['d_ff'])\n",
        "\n",
        "    # Log model configuration in wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "    model, optimizer, initial_epoch, global_step = load_model(config, device, model, optimizer)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_dataloader)\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['inputs_ids'].to(device)\n",
        "            targets = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            logits, loss = model(encoder_input, targets=targets)\n",
        "\n",
        "            total_loss += loss.item()  # Accumulate loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # Log batch loss to wandb\n",
        "            wandb.log({\"batch_loss\": loss.item(), \"global_step\": global_step})\n",
        "\n",
        "            batch_iterator.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "        # Epoch-level logging\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch} | Avg Loss: {round(avg_loss, 2)}\")\n",
        "\n",
        "        # Log average loss for epoch to wandb\n",
        "        wandb.log({\"average_loss\": avg_loss, \"epoch\": epoch})\n",
        "\n",
        "        # Model checkpointing\n",
        "        model_filename = config['model_file_path']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step,\n",
        "        }, model_filename)\n",
        "\n",
        "        validate(model, val_dataloader, device, epoch)\n",
        "\n",
        "def validate(model, val_dataloader, device, epoch):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_batches = len(val_dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['inputs_ids'].to(device)\n",
        "            targets = batch['labels'].to(device)\n",
        "            _, val_loss = model(input_ids, targets=targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_batches\n",
        "    print(f\"Validation Loss (Epoch {epoch}): {round(avg_val_loss, 2)}\")\n",
        "\n",
        "    # Log validation loss to wandb\n",
        "    wandb.log({\"validation_loss\": avg_val_loss, \"epoch\": epoch})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbUaKraacpj",
        "outputId": "b29b2d0b-6270-402d-e07a-56e10243b618"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "config = get_config()\n",
        "tokenizer = get_tokenizer(config)\n",
        "\n",
        "user_token_id = tokenizer.token_to_id('[USER]')\n",
        "bot_token_id = tokenizer.token_to_id('[BOT]')\n",
        "\n",
        "# Initialize the model\n",
        "model = TCLM(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    seq_len=config['seq_len'],\n",
        "    d_model=config['d_model'],\n",
        "    N=config['n_layers'],\n",
        "    h=config['head'],\n",
        "    dropout=config['dropout'],\n",
        "    d_ff=config['d_ff']\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "model, _, _, _ = load_model(config, config['device'], model, optimizer)\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\" or user_input == \"\":\n",
        "        break\n",
        "\n",
        "    conversation_history.append(user_input)\n",
        "    input_sequence = [user_token_id]\n",
        "    user_input_ids = tokenizer.encode(user_input).ids\n",
        "    input_sequence += user_input_ids\n",
        "\n",
        "    # If conversation history exceeds a certain length, truncate it\n",
        "    if len(conversation_history) > 5:  # Adjust the number based on context length needed\n",
        "        conversation_history = conversation_history[-5:]\n",
        "\n",
        "    # Append previous turns to the input sequence\n",
        "    for i in range(len(conversation_history)):\n",
        "        if i % 2 == 0:  # User turn\n",
        "            input_sequence += [user_token_id] + tokenizer.encode(conversation_history[i]).ids\n",
        "        else:  # Bot turn\n",
        "            input_sequence += [bot_token_id] + tokenizer.encode(conversation_history[i]).ids\n",
        "\n",
        "    # Convert to tensor and send to device\n",
        "    try:\n",
        "        input_tensor = torch.tensor([input_sequence]).to(config['device'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error while converting to tensor: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Generate response from the model\n",
        "    generated_sequence = model.generate(\n",
        "        input_tensor,\n",
        "        max_new_tokens=20,\n",
        "        seq_len=config['seq_len'],\n",
        "        temperature=config['temperature'],\n",
        "        top_k=config['top_k']\n",
        "    )\n",
        "\n",
        "    predicted_text = tokenizer.decode(generated_sequence[0].cpu().numpy())\n",
        "    conversation_history.append(predicted_text)\n",
        "\n",
        "    print(\"Omnira:\", predicted_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
